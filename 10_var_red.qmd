# Técnicas de Redução de Variância


A simulação de Monte Carlo é utilizada para estimar esperanças matemáticas e calcular integrais numéricas. No entanto, a precisão do método pode ser limitada pela variância das amostras geradas. As técnicas de redução de variância visam minimizar essa variabilidade sem aumentar significativamente o número de amostras.


## Técnica 1: Variáveis Antitéticas

### Motivação

Queremos estimar uma integral $\theta = E[X]$. O estimador de Monte Carlo usual é:
$$
\hat{\theta} = \frac{1}{n} \sum_{i=1}^{n} X_i
$$
onde $X_1, ..., X_n$ são variáveis aleatórias iid com média $\theta$. A variância desse estimador é dada por
$$
\text{Var}(\hat{\theta}) = \frac{\text{Var}(X)}{n}
$$

Se conseguirmos construir um conjunto de amostras correlacionadas negativamente com a amostra de $X$'s, podemos reduzir essa variância.

### Descrição

A ideia principal das **variáveis antitéticas** é gerar pares de amostras $X_i$ e $Y_i$ que sejam correlacionadas negativamente. Se tivermos uma segunda amostra $Y_1, ..., Y_n$, não necessariamente independente de $X$, com
$$
E[Y] = E[X] = \theta
$$
e
$$
V[Y] = V[X] = \sigma^2,
$$
então podemos definir um novo estimador:
$$
\hat{\theta}_A = \frac{1}{2n} \sum_{i=1}^{n} (X_i + Y_i)
$$
cuja
esperança é
$$
\text{E}(\hat{\theta}_A)=\theta $$
e variância é
$$
\text{Var}(\hat{\theta}_A) = \frac{1}{4n} \sum_{i=1}^{n} \left( \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X,Y) \right)
$$
ou, simplificando,
$$
\text{Var}(\hat{\theta}_A) = \frac{\text{Var}(X)}{n} \left( \frac{1 + \rho}{2} \right)
$$
onde $\rho = \text{Corr}(X, Y)$. Se $\rho < 0$, então $\text{Var}(\hat{\theta}_A) < \text{Var}(\hat{\theta})$, tornando o estimador **mais eficiente**.

### Algoritmo

Uma maneira de gerar variáveis negativamente correlacionadas é a partir
de distribuições uniformes. Para isso, vamos assumir que 
$X \sim h(U)$, para algum $h$ conhecido, em que $U \sim U(0,1)$. Sabemos, pelo método
da transformação inversa, que sempre é possível encontrar $h$ com essa propriedade.

A ideia chave é então usar que $1-U$ também tem distribuição $U(0,1)$.
Assim  $h(1-U)$ tem a mesma distribuição de $X$ e, em particular, a mesma média $\theta$.
Além disso, pode-se mostrar que se $h$ é monótono, $h(U)$ tem correlação
negativa com $h(1-U)$.

O método das variáveis antitéticas pode ser implementado da seguinte forma:

1. Geramos $n$ variáveis aleatórias uniformes $U_i \sim U(0,1)$.
2. Definimos $X_i = h(U_i)$.
3. Para cada $U_i$, também geramos $Y_i = h(1 - U_i)$.
4. Construímos o estimador antitético:
$$
\hat{\theta}_A = \frac{1}{2n} \sum_{i=1}^{n} (X_i + Y_i)
$$


---

### Exemplo: Aproximação de uma Integral

Queremos estimar a seguinte integral:

$$
I = \int_0^1 \log(1 + x^2) \, dx
$$

Sabemos que podemos reescrever essa integral como uma esperança matemática:

$$
I = E[\log(1 + X^2)], \quad X \sim U(0,1)
$$

O algoritmo é:

1. Geramos $n$ variáveis uniformes $U_i \sim U(0,1)$.
2. Definimos $X_i = -\log(U_i)$.
3. Definimos $Y_i = -\log(1 - U_i)$.
4. Construímos:

$$
Z_i = \frac{1}{2} \left( \log(1 + X_i^2) + \log(1 + Y_i^2) \right)
$$


---

::: {.panel-tabset group="language"}
# R

```{r}
set.seed(42)
n <- 50000

# Geração das amostras
u <- runif(n, 0, 1)
x <- -log(u)
y <- -log(1 - u)

# Cálculo das estimativas
theta.hatx <- log(1 + x^2)
theta.haty <- log(1 + y^2)
theta.hatz <- (theta.hatx + theta.haty) / 2

# Média das estimativas
mean_values <- c(mean(theta.hatx), mean(theta.haty), mean(theta.hatz))
cat("Médias das estimativas:\n")
print(mean_values)
```


# Python
```{python}
import numpy as np

np.random.seed(42)
n = 50000

# Geração das amostras
u = np.random.uniform(0, 1, n)
x = -np.log(u)
y = -np.log(1 - u)

# Cálculo das estimativas
theta_hatx = np.log(1 + x**2)
theta_haty = np.log(1 + y**2)
theta_hatz = (theta_hatx + theta_haty) / 2

# Média das estimativas
mean_values = [np.mean(theta_hatx), np.mean(theta_haty), np.mean(theta_hatz)]
print("Médias das estimativas:")
print(mean_values)
```
:::

A seguir, repetiremos esse processo várias vezes para avaliar as variâncias dos diferentes estimadores.


::: {.panel-tabset group="language"}
# R

```{r}
set.seed(42)
n <- 50000
B <- 1000  # Número de repetições

theta.hatx <- numeric(B)
theta.haty <- numeric(B)
theta.hatz <- numeric(B)

for (b in 1:B) {
  u <- runif(n, 0, 1)
  x <- -log(u)
  y <- -log(1 - u)
  
  theta.hatx[b] <- mean(log(1 + x^2))
  theta.haty[b] <- mean(log(1 + y^2))
  theta.hatz[b] <- mean((log(1 + x^2) + log(1 + y^2)) / 2)
}

# Cálculo das variâncias das estimativas
var_values <- c(var(theta.hatx), var(theta.haty), var(theta.hatz))
cat("Variâncias das estimativas:\n")
print(var_values)
```


# Python
```{python}
import numpy as np

np.random.seed(42)
n = 50000
B = 1000  # Número de repetições

theta_hatx = np.zeros(B)
theta_haty = np.zeros(B)
theta_hatz = np.zeros(B)

for b in range(B):
    u = np.random.uniform(0, 1, n)
    x = -np.log(u)
    y = -np.log(1 - u)
    
    theta_hatx[b] = np.mean(np.log(1 + x**2))
    theta_haty[b] = np.mean(np.log(1 + y**2))
    theta_hatz[b] = np.mean((np.log(1 + x**2) + np.log(1 + y**2)) / 2)

# Cálculo das variâncias das estimativas
var_values = [np.var(theta_hatx), np.var(theta_haty), np.var(theta_hatz)]
print("Variâncias das estimativas:")
print(var_values)
```
:::

## Técnica 2: Uso de Variáveis de Controle

### Motivação
Ao estimar o valor esperado de uma variável aleatória $X$, muitas vezes queremos melhorar a precisão da estimativa reduzindo a variância associada. Para isso, podemos introduzir uma variável auxiliar $Y$, conhecida como **variável de controle**, que possui um valor esperado conhecido, $\mathbb{E}[Y] = \mu$, e uma relação com $X$ que nos ajuda a diminuir a variância.

### Explicação do Método
Dado que queremos estimar $\theta = \mathbb{E}[X]$, definimos uma nova estimativa ajustada:

$$
Z = X + c(Y - \mu),
$$

onde $c$ é uma constante a ser escolhida para minimizar a variância de $Z$. Ao calcular a expectativa de $Z$, temos:

$$
\mathbb{E}[Z] = \mathbb{E}[X + c(Y - \mu)] = \mathbb{E}[X] + c \cdot (\mathbb{E}[Y] - \mu) = \mathbb{E}[X] = \theta.
$$

Assim, a nova estimativa é não viesada. Agora, calculamos a variância:

$$
\text{Var}[Z] = \text{Var}[X + c(Y - \mu)] = \text{Var}[X] + c^2 \text{Var}[Y] + 2c \cdot \text{Cov}(X, Y).
$$

Para minimizar essa variância, derivamos em relação a $c$ e encontramos o valor ótimo:

$$
c^* = -\frac{\text{Cov}(X, Y)}{\text{Var}(Y)}.
$$

Substituindo $c^*$ na expressão da variância, obtemos:

$$
\text{Var}[Z] = \text{Var}[X] - \frac{[\text{Cov}(X, Y)]^2}{\text{Var}(Y)}.
$$

Essa fórmula mostra que o uso da variável de controle reduz a variância de forma significativa.

### Redução da Variância
Usando a correlação $\text{Corr}(X, Y) = \text{Cov}(X, Y)/\sqrt{\text{Var}(X)\text{Var}(Y)}$, podemos reescrever a variância como:

$$
\text{Var}[Z] = \text{Var}[X] \cdot \left(1 - [\text{Corr}(X, Y)]^2\right).
$$

A redução percentual da variância é dada por:

$$
\frac{\text{Var}[Z]}{\text{Var}[X]} = 1 - [\text{Corr}(X, Y)]^2.
$$

Quanto maior a correlação entre $X$ e $Y$, maior será a redução da variância.

### Estimativa Prática
Na prática, as quantidades $\text{Cov}(X, Y)$ e $\text{Var}(Y)$ podem ser estimadas a partir de uma amostra gerada via Monte Carlo. Suponha que temos $n$ amostras de $X$ e $Y$: $X_1, \dots, X_n$ e $Y_1, \dots, Y_n$. Então:

$$
\widehat{\text{Cov}}(X, Y) = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y}),
$$

$$
\widehat{\text{Var}}(Y) = \frac{1}{n-1} \sum_{i=1}^n (Y_i - \bar{Y})^2.
$$

Com esses valores, aproximamos $c^*$ por:

$$
\widehat{c}^* = -\frac{\widehat{\text{Cov}}(X, Y)}{\widehat{\text{Var}}(Y)}.
$$

O estimador final é dado por:

$$
Z = X + \widehat{c}^* (Y - \bar{Y}).
$$

### Exemplo
Considere $X = e^U$, onde $U \sim \text{Unif}(0, 1)$. Vamos usar $Y = U$ como variável de controle. Sabemos que $\mathbb{E}[U] = 0.5$. Calculamos:

$$
\text{Cov}(e^U, U) = \mathbb{E}[e^U U] - \mathbb{E}[U]\mathbb{E}[e^U].
$$

Após as integrações necessárias, encontramos:

$$
\text{Cov}(e^U, U) = 0.14086.
$$

A variância de $e^U + c^*(U - 0.5)$ é:

$$
\text{Var}[e^U + c^*(U - 0.5)] = 0.2420 - 12(0.14086^2) = 0.0039.
$$

A redução de variância é:

$$
1 - \frac{0.0039}{0.2420} = 0.9838 \quad \text{(98.38%)}.
$$

Isso demonstra como o uso de variáveis de controle pode reduzir drasticamente a variância na estimativa.


---

## Exercícios

**Exercício 1**. Implemente a técnica das variáveis antitéticas para estimar $E[\sin(X)]$ onde $X \sim U(0, \pi)$. Compare a variância do estimador com o estimador usual.

**Exercício 2**. Considere a integral:

$$
\theta = \int_{0}^{\pi/4} \int_{0}^{\pi/4} x^2 y^2 \sin(x + y) \log(x + y) \, dx \, dy.
$$

(a) Implemente o método de Monte Carlo para aproximar o valor da integral acima.  
(b) Aproxime a integral utilizando $X^2 Y^2$ como variável de controle.  
(c) Compare a variância dos estimadores obtidos nos itens (a) e (b).
