# Técnicas de Redução de Variância


A simulação de Monte Carlo é utilizada para estimar esperanças matemáticas e calcular integrais numéricas. No entanto, a precisão do método pode ser limitada pela variância das amostras geradas. As técnicas de redução de variância visam minimizar essa variabilidade sem aumentar significativamente o número de amostras.


## Técnica 1: Variáveis Antitéticas

### Motivação

Queremos estimar uma integral $\theta = E[X]$. O estimador de Monte Carlo usual é:
$$
\hat{\theta} = \frac{1}{n} \sum_{i=1}^{n} X_i
$$
onde $X_1, ..., X_n$ são variáveis aleatórias iid com média $\theta$. A variância desse estimador é dada por
$$
\text{Var}(\hat{\theta}) = \frac{\text{Var}(X)}{n}
$$

Se conseguirmos construir um conjunto de amostras correlacionadas negativamente com a amostra de $X$'s, podemos reduzir essa variância.

### Descrição

A ideia principal das **variáveis antitéticas** é gerar pares de amostras $X_i$ e $Y_i$ que sejam correlacionadas negativamente. Se tivermos uma segunda amostra $Y_1, ..., Y_n$, não necessariamente independente de $X$, com
$$
E[Y] = E[X] = \theta
$$
e
$$
V[Y] = V[X] = \sigma^2,
$$
então podemos definir um novo estimador:
$$
\hat{\theta}_A = \frac{1}{2n} \sum_{i=1}^{n} (X_i + Y_i)
$$
cuja
esperança é
$$
\text{E}(\hat{\theta}_A)=\theta $$
e variância é
$$
\text{Var}(\hat{\theta}_A) = \frac{1}{4n} \sum_{i=1}^{n} \left( \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X,Y) \right)
$$
ou, simplificando,
$$
\text{Var}(\hat{\theta}_A) = \frac{\text{Var}(X)}{n} \left( \frac{1 + \rho}{2} \right)
$$
onde $\rho = \text{Corr}(X, Y)$. Se $\rho < 0$, então $\text{Var}(\hat{\theta}_A) < \text{Var}(\hat{\theta})$, tornando o estimador **mais eficiente**.

### Algoritmo

Uma maneira de gerar variáveis negativamente correlacionadas é a partir
de distribuições uniformes. Para isso, vamos assumir que 
$X \sim h(U)$, para algum $h$ conhecido, em que $U \sim U(0,1)$. Sabemos, pelo método
da transformação inversa, que sempre é possível encontrar $h$ com essa propriedade.

A ideia chave é então usar que $1-U$ também tem distribuição $U(0,1)$.
Assim  $h(1-U)$ tem a mesma distribuição de $X$ e, em particular, a mesma média $\theta$.
Além disso, pode-se mostrar que se $h$ é monótono, $h(U)$ tem correlação
negativa com $h(1-U)$.

O método das variáveis antitéticas pode ser implementado da seguinte forma:

1. Geramos $n$ variáveis aleatórias uniformes $U_i \sim U(0,1)$.
2. Definimos $X_i = h(U_i)$.
3. Para cada $U_i$, também geramos $Y_i = h(1 - U_i)$.
4. Construímos o estimador antitético:
$$
\hat{\theta}_A = \frac{1}{2n} \sum_{i=1}^{n} (X_i + Y_i)
$$


---

## Exemplo: Aproximação de uma Integral

Queremos estimar a seguinte integral:

$$
I = \int_0^1 \log(1 + x^2) \, dx
$$

Sabemos que podemos reescrever essa integral como uma esperança matemática:

$$
I = E[\log(1 + X^2)], \quad X \sim U(0,1)
$$

O algoritmo é:

1. Geramos $n$ variáveis uniformes $U_i \sim U(0,1)$.
2. Definimos $X_i = -\log(U_i)$.
3. Definimos $Y_i = -\log(1 - U_i)$.
4. Construímos:

$$
Z_i = \frac{1}{2} \left( \log(1 + X_i^2) + \log(1 + Y_i^2) \right)
$$


---

::: {.panel-tabset group="language"}
# R

```{r}
set.seed(42)
n <- 50000

# Geração das amostras
u <- runif(n, 0, 1)
x <- -log(u)
y <- -log(1 - u)

# Cálculo das estimativas
theta.hatx <- log(1 + x^2)
theta.haty <- log(1 + y^2)
theta.hatz <- (theta.hatx + theta.haty) / 2

# Média das estimativas
mean_values <- c(mean(theta.hatx), mean(theta.haty), mean(theta.hatz))
cat("Médias das estimativas:\n")
print(mean_values)
```


# Python
```{python}
import numpy as np

np.random.seed(42)
n = 50000

# Geração das amostras
u = np.random.uniform(0, 1, n)
x = -np.log(u)
y = -np.log(1 - u)

# Cálculo das estimativas
theta_hatx = np.log(1 + x**2)
theta_haty = np.log(1 + y**2)
theta_hatz = (theta_hatx + theta_haty) / 2

# Média das estimativas
mean_values = [np.mean(theta_hatx), np.mean(theta_haty), np.mean(theta_hatz)]
print("Médias das estimativas:")
print(mean_values)
```
:::

A seguir, repetiremos esse processo várias vezes para avaliar as variâncias dos diferentes estimadores.


::: {.panel-tabset group="language"}
# R

```{r}
set.seed(42)
n <- 50000
B <- 1000  # Número de repetições

theta.hatx <- numeric(B)
theta.haty <- numeric(B)
theta.hatz <- numeric(B)

for (b in 1:B) {
  u <- runif(n, 0, 1)
  x <- -log(u)
  y <- -log(1 - u)
  
  theta.hatx[b] <- mean(log(1 + x^2))
  theta.haty[b] <- mean(log(1 + y^2))
  theta.hatz[b] <- mean((log(1 + x^2) + log(1 + y^2)) / 2)
}

# Cálculo das variâncias das estimativas
var_values <- c(var(theta.hatx), var(theta.haty), var(theta.hatz))
cat("Variâncias das estimativas:\n")
print(var_values)
```


# Python
```{python}
import numpy as np

np.random.seed(42)
n = 50000
B = 1000  # Número de repetições

theta_hatx = np.zeros(B)
theta_haty = np.zeros(B)
theta_hatz = np.zeros(B)

for b in range(B):
    u = np.random.uniform(0, 1, n)
    x = -np.log(u)
    y = -np.log(1 - u)
    
    theta_hatx[b] = np.mean(np.log(1 + x**2))
    theta_haty[b] = np.mean(np.log(1 + y**2))
    theta_hatz[b] = np.mean((np.log(1 + x**2) + np.log(1 + y**2)) / 2)

# Cálculo das variâncias das estimativas
var_values = [np.var(theta_hatx), np.var(theta_haty), np.var(theta_hatz)]
print("Variâncias das estimativas:")
print(var_values)
```
:::


---

## Exercícios

**Exercício 1**. Implemente a técnica das variáveis antitéticas para estimar $E[\sin(X)]$ onde $X \sim U(0, \pi)$. Compare a variância do estimador com o estimador usual.
